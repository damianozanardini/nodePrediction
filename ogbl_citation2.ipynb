{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ogbl-citation2 graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This is the original description of the dataset. Clearly, **our goal is to set up a completely different prediction task**.\n",
    "\n",
    "**Graph**: The ogbl-citation2 dataset is a directed graph, representing the citation network between a subset of papers extracted from MAG. Dach node is a paper with **128-dimensional word2vec features** that summarizes its title and abstract, and each directed edge indicates that **one paper cites another**. All nodes also come with meta-information indicating the year the corresponding paper was published.\n",
    "\n",
    "**Prediction task**: The task is to predict missing citations given existing citations. Specifically, for each source paper, two of its references are randomly dropped, and we would like the model to rank the missing two references higher than 1,000 negative reference candidates. The negative references are randomly-sampled from all the previous papers that are not referenced by the source paper. The evaluation metric is Mean Reciprocal Rank (MRR), where the reciprocal rank of the true reference among the negative candidates is calculated for each source paper, and then the average is taken over all source papers.\n",
    "\n",
    "**Dataset splitting**: We split the edges according to time, in order to simulate a realistic application in citation recommendation (e.g., a user is writing a new paper and has already cited several existing papers, but wants to be recommended additional references). To this end, we use the most recent papers (those published in 2019) as the source papers for which we want to recommend the references. For each source paper, we drop two papers from its references—the resulting two dropped edges (pointing from the source paper to the dropped papers) are used respectively for validation and testing. All the rest of the edges are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damiano/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ogb\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "import random\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import subgraph\n",
    "import torch_geometric.utils as utils\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from my_utils.ipynb\n",
      "importing Jupyter notebook from ogbl_citation2.ipynb\n"
     ]
    }
   ],
   "source": [
    "import my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_name():\n",
    "    return graph_name\n",
    "\n",
    "# TO-DO: compute it from graph_name by replacing '-' with '_'\n",
    "def get_module_name():\n",
    "    return module_name\n",
    "\n",
    "def get_n_nodes():\n",
    "    return n_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##### EXECUTE ONLY IF THE ORIGINAL GRAPH HAS TO BE DOWNLOADED (it takes time) ##\n",
    "def download_graph(graph_name):\n",
    "    dataset = PygLinkPropPredDataset(name=graph_name)\n",
    "    graph_0 = dataset[0]\n",
    "\n",
    "    print(f'{graph_name} has {graph_0.num_nodes} nodes and {graph_0.num_edges} edges, with an average (incoming) node degree of {graph_0.num_edges / graph_0.num_nodes}')\n",
    "    print(f'Each node has {len(graph_0.x[0])} features')\n",
    "\n",
    "    # G = convert.to_networkx(graph_0, to_undirected=False)\n",
    "    # Too big to be drawed\n",
    "    # nx.draw(G)\n",
    "\n",
    "    return (graph_0)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##### EXECUTE ONLY IF THE ORIGINAL GRAPH HAS TO BE REDUCED #####################\n",
    "def compute_reduced_graph(dir,num_nodes_to_sample):\n",
    "    graph0 = download_graph(get_graph_name())\n",
    "    mask, graph = my_utils.reduce_graph(\n",
    "                            graph=graph0,\n",
    "                            n_nodes=num_nodes_to_sample,\n",
    "                            dense=True,\n",
    "                            reverse=False,\n",
    "                            clip=True,\n",
    "                            relabel_nodes=True)\n",
    "\n",
    "    print(f'The reduced version of {get_graph_name()} has {graph.num_nodes} nodes and {graph.num_edges} edges, with an average (incoming) node degree of {graph.num_edges / graph.num_nodes}')\n",
    "    print(f'Each node has {len(graph.x[0])} features')\n",
    "\n",
    "    file = my_utils.get_path_of_reduced_graph(dir,get_module_name(),num_nodes_to_sample)\n",
    "    print(f\"loading from {file}\")\n",
    "    torch.save(graph, file)\n",
    "\n",
    "def get_reduced_graph(dir, sampled_nodes, node_features='original'):\n",
    "    file = my_utils.get_path_of_reduced_graph(dir,get_module_name(),sampled_nodes)\n",
    "    print(f\"loading from {file}\")\n",
    "    graph = torch.load(file) #, map_location=torch.device(device)).to(device)\n",
    "\n",
    "    # Optionally, we can use newly-computed node features instead of the original 128-dimensional vector\n",
    "    # This choice leads to a clear information LEAKAGE: in some sense, features for test nodes would\n",
    "    # inherit information about the graph connectivity (that is, we are giving information that we are supposed to predict)\n",
    "    if node_features == 'stats':\n",
    "        print(\"Taking newly-generated node features instead of the original ones\")\n",
    "        G = utils.convert.to_networkx(graph, to_undirected=False)\n",
    "        # compute node stats using existing algorithms\n",
    "        pagerank = nx.algorithms.link_analysis.pagerank_alg.pagerank(G)\n",
    "        clustering_coef = nx.algorithms.cluster.clustering(G)\n",
    "        betweenness_centrality = nx.betweenness_centrality(G, k=50)\n",
    "        degree = G.degree()\n",
    "        # create initial node features from that\n",
    "        aug_emb = torch.ones(graph.num_nodes, 5, dtype=torch.float64) # .to(device)\n",
    "        for i in range(graph.num_nodes):\n",
    "            aug_emb[i][0] = degree[i]\n",
    "            aug_emb[i][1] = pagerank[i]\n",
    "            aug_emb[i][2] = betweenness_centrality[i]\n",
    "            aug_emb[i][3] = pagerank[i]\n",
    "            aug_emb[i][4] = 1.0\n",
    "            aug_emb = aug_emb.float()\n",
    "        graph = Data(x=aug_emb, edge_index=graph.edge_index)\n",
    "\n",
    "    if node_features == 'dummy': # TO-DO: to run tests with this choice gives some error; double-check\n",
    "        aug_emb = torch.ones(graph.num_nodes, 3, dtype=torch.float64) # .to(device)\n",
    "        aug_emb = aug_emb.float()\n",
    "        graph = Data(x=aug_emb, edge_index=graph.edge_index) # .to(device)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating other kinds of graphs starting from the citation graph\n",
    "\n",
    "# Starting from the undirected version of the (big) citation graph, and one\n",
    "# randomly-chosen node, builds a connected graph with ~n_nodes\n",
    "# nodes, together with the original connections between them\n",
    "#\n",
    "# Needs the original graph 'graph_0' to have been previously loaded\n",
    "def get_ego_network_un(graph_0, n_nodes):\n",
    "    # First, the undirected version of the original graph is computed\n",
    "    edge_index_un = utils.to_undirected(graph_0.edge_index)\n",
    "    # This indicates the minimum and maximum number of nodes allowed\n",
    "    min_ratio, max_ratio = [2/3,3/2]\n",
    "    final_nodes = sys.maxsize\n",
    "    # we keep generating ego networks randomly until a reasonable size is obtained (less than double the expected nodes)\n",
    "    while (final_nodes>n_nodes*max_ratio):\n",
    "        # First gets a randomly-chosen node\n",
    "        initial_node = random.randint(0,graph_0.num_nodes-1)\n",
    "        print(f\"Starting from node {initial_node}...\")\n",
    "        nodes = [initial_node]\n",
    "        k = 1\n",
    "        # Generates k_hop subgraph until the node number is enough\n",
    "        while len(nodes)<(n_nodes*min_ratio):\n",
    "            nodes, edge_index, mapping, edge_mask = utils.k_hop_subgraph([initial_node],\n",
    "                                                             k,\n",
    "                                                             edge_index_un,\n",
    "                                                             relabel_nodes=True)\n",
    "            print(f\"   k: {k} -> {len(nodes)} nodes\")\n",
    "            k = k+1\n",
    "        final_nodes = len(nodes)\n",
    "    # Filters node features\n",
    "    mask_n = torch.zeros(graph_0.num_nodes).byte()\n",
    "    for n in nodes:\n",
    "        mask_n[n] = True\n",
    "    x = graph_0.x[mask_n]\n",
    "    g = Data(x=x,edge_index=edge_index)\n",
    "    print(f\"Ego network generated with {g.num_nodes} nodes and {g.num_edges} edges\")\n",
    "    return g\n",
    "\n",
    "def get_random_features(n_nodes,num_node_features,device):\n",
    "    # random values in [0,1]\n",
    "    x = torch.rand(n_nodes,num_node_features).to(device)\n",
    "    # map values to [-1,1]\n",
    "    x = (x-0.5)*2\n",
    "    return x\n",
    "\n",
    "# This takes a reduced graph with N*train_ratio nodes, and adds the remaining\n",
    "# N*(1-train_ratio) nodes by using Barabási-Albert. The idea is that new nodes\n",
    "# are the test nodes\n",
    "def get_sampled_plus_barabasi_albert_graph(n_nodes,train_ratio,num_node_features,device):\n",
    "    n_train_nodes = round(n_nodes*train_ratio)\n",
    "    n_test_nodes = n_nodes-n_train_nodes\n",
    "\n",
    "    graph0 = get_reduced_graph(sampled_nodes=n_train_nodes, node_features='original')\n",
    "    G0 = utils.convert.to_networkx(graph0)\n",
    "    G = random_graphs.barabasi_albert_graph(n_nodes,round(1.5*graph0.num_edges/graph0.num_nodes),initial_graph=G0) # the second argument is meant to produce a similar number of edges as the dense subgraph\n",
    "\n",
    "    x = get_random_features(n_nodes,num_node_features,device)\n",
    "\n",
    "    graph = utils.convert.from_networkx(G).to(device)\n",
    "    graph.x = x\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(dir,n_nodes,node_features_type):\n",
    "    if n_nodes >= get_n_nodes():\n",
    "        # No need to reduce the graph\n",
    "        graph0 = download_graph(get_graph_name())\n",
    "    else:\n",
    "        # Reduction of the graph is needed\n",
    "        path = my_utils.get_path_of_reduced_graph(dir,get_module_name(),n_nodes)\n",
    "        if not os.path.exists(path):\n",
    "            compute_reduced_graph(dir,n_nodes)\n",
    "        graph0 = get_reduced_graph(dir,n_nodes,node_features_type)\n",
    "    n_edges = graph0.num_edges\n",
    "    num_features = len(graph0.x[0])\n",
    "\n",
    "    return graph0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'ogbl-citation2'\n",
    "module_name = 'ogbl_citation2'\n",
    "\n",
    "n_nodes = 2927963"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
