{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianozanardini/nodePrediction/blob/main/NodePrediction_submit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOnGH8jsl5NL"
      },
      "source": [
        "# New node prediction\n",
        "\n",
        "## Damiano Zanardini and Emilio Serrano\n",
        "\n",
        "This work has been published as [New node prediction: a novel learning task for Graph Neural Networks](https://www.sciencedirect.com/science/article/pii/S0925231224012451?dgcid=coauthor).\n",
        "\n",
        "This notebook wants to be an implementation of the framework that is flexible in terms of the graph it works on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W57vvc3jEscl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzB9tYiDGcts"
      },
      "source": [
        "### Installs and imports\n",
        "\n",
        "We first will install [PyG](https://pyg.org/) (PyTorch Geometric), together with [ogb](https://github.com/snap-stanford/ogb) (Open Graph Benchmark) and the usual packages.\n",
        "\n",
        "Due to unknown reasons, to install PyG and related packages on Colab may lead to some unexpected behavior (namely, it may take nearly one hour instead of the usual 30/40 seconds). Because of this, we include a couple of code cells with code for installing PyG that worked properly at times in the past."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEmIi1CJQX6",
        "outputId": "6a5eae31-4ecb-4462-a326-95a03259af68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-dqhte3o9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-dqhte3o9\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit c6f3a55dff40992947ba15e34925cb3333a7c351\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.4.0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=971194 sha256=665946a5a383a585c7f7ba02752bf07a0bfe2c669e0a31b2c89490115248691b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h6eri43d/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt20cu118\n"
          ]
        }
      ],
      "source": [
        "# This is the better-working approach so far\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['TORCH_DOWNPATH'] = \"https://pytorch-geometric.com/whl/\" # \"https://data.pyg.org/whl/\"\n",
        "\n",
        "!pip uninstall torch-scatter torch-geometric --y\n",
        "\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install torch-scatter -f ${TORCH_DOWNPATH}torch-${TORCH}.html\n",
        "#!pip install torch-sparse -f ${TORCH_DOWNPATH}torch-${TORCH}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL3GZ4cyZ8eP"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "################################################################################\n",
        "# [NOT TO BE RUN NORMALLY] Alternatives for installing PyG (1) #################\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "!pip install torch_geometric\n",
        "\n",
        "# Optional dependencies:\n",
        "!pip install torch_scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Tgj6bFGf5E"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "################################################################################\n",
        "# [NOT TO BE RUN NORMALLY] Alternatives for installing PyG (2) #################\n",
        "\n",
        "import torch\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))\n",
        "\n",
        "#down_url = \"https://data.pyg.org/whl/\"\n",
        "down_url = \"https://pytorch-geometric.com/whl/\"\n",
        "\n",
        "# Installing torch-scatter, torch-sparse and torch-geometric is somehow painful\n",
        "# Sometimes it is necessary to change the download urls\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric --y\n",
        "\n",
        "!pip install torch-scatter -f {down_url}torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f {down_url}torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "#!pip install torch-geometric \\\n",
        "#  torch-sparse \\\n",
        "#  torch-scatter \\\n",
        "#  -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "\n",
        "#!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}.html\n",
        "#!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}.html\n",
        "#!pip install torch-geometric\n",
        "\n",
        "#!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0avqbBNfui",
        "outputId": "8d27acf1-2373-406b-c06c-8022bde58878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=92a420974c5f3878d8e19a710bb4628b584f5d98edcd5a57694898d97e0d65bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.0.1-py3-none-any.whl (729 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.2/729.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Collecting lightning-utilities>=0.7.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.7.0->torchmetrics) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ogb\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from networkx.generators import random_graphs\n",
        "\n",
        "import random\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_geometric.utils as utils\n",
        "from torch_geometric.nn import GCNConv, ClusterGCNConv, GATConv, SAGEConv\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import subgraph\n",
        "\n",
        "import math\n",
        "from statistics import mean\n",
        "import calendar\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import PearsonCorrCoef\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Qb6WL87gMC"
      },
      "source": [
        "### Global vars/options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFejKNq57glO",
        "outputId": "464a89e1-6ea3-4053-eb7f-29275f781da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is cuda\n"
          ]
        }
      ],
      "source": [
        "# Experiments are run on cuda (if available) or cpu\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device is {device}')\n",
        "\n",
        "# Local Google Drive directory where experimental results are stored\n",
        "dir = \"/content/drive/MyDrive/Colab Notebooks/NodePrediction/\"\n",
        "\n",
        "# The original graph\n",
        "graph_name = 'ogbl-citation2'\n",
        "\n",
        "# Given a graph, the proportion of train nodes vs. test nodes\n",
        "# (80%/20% is the split that is used in the paper)\n",
        "train_ratio = 0.8\n",
        "\n",
        "# Whether computing times for the main training steps are shown\n",
        "get_times = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZMbagpDF8ew"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YyCH4FJZF6bt"
      },
      "outputs": [],
      "source": [
        "# Returns the n_max most frequent items from a tensor\n",
        "def get_most_frequent(my_tensor,n_max):\n",
        "  bincount = torch.bincount(my_tensor)\n",
        "  bincount_s,indices = torch.sort(bincount,descending=True)\n",
        "  return indices[:n_max]\n",
        "\n",
        "# Returns a reduced version of the graph with only n_nodes nodes\n",
        "# - if dense: nodes with highest degree are taken (otherwise, randomly chosen)\n",
        "# - if reverse: edges are reversed\n",
        "# - if clip: the rest of the nodes are NOT kept in the reduced graph\n",
        "# - if relabel_nodes: node ids are relabeled (from 0 to n_nodes-1)\n",
        "def reduce_graph(graph,n_nodes,dense=True,reverse=False,clip=False,relabel_nodes=False):\n",
        "  if dense:\n",
        "    # selects the nodes with a higher in-degree\n",
        "    nodes = get_most_frequent(graph.edge_index[1],n_nodes)\n",
        "  else:\n",
        "    # selects nodes randomly\n",
        "    nodes = random.sample(range(graph.num_nodes), n_nodes)\n",
        "\n",
        "  edges = subgraph(subset=nodes,edge_index=graph.edge_index,relabel_nodes=relabel_nodes)\n",
        "  if reverse:\n",
        "    r_edge_index = torch.stack([edges[0][1],edges[0][0]])\n",
        "  else:\n",
        "    r_edge_index = edges[0]\n",
        "\n",
        "  mask_n = torch.zeros(graph.num_nodes).byte()\n",
        "  for n in nodes:\n",
        "    mask_n[n] = True\n",
        "  if clip:\n",
        "    x = graph.x[mask_n]\n",
        "  else:\n",
        "    x = graph.x\n",
        "\n",
        "  return mask_n, Data(x=x,edge_index=r_edge_index).to(device) #,y=graph.y[mask_n])\n",
        "\n",
        "# For each experiment, a line is written to a local Excel file that contains the main figures:\n",
        "# Type of architecture, number of nodes, hyper-parameters, training time, accuracy, hits@k, MRR...\n",
        "def write_line_to_xlsx(output):\n",
        "  current_GMT = time.gmtime()\n",
        "  time_stamp = calendar.timegm(current_GMT)\n",
        "  time_stamp = datetime.utcfromtimestamp(time_stamp).strftime('%Y-%m-%d %H:%M:%S')\n",
        "  records = [tuple([time_stamp] + list(output.values()))]\n",
        "\n",
        "  wb = load_workbook(dir + \"tmp_results.xlsx\")\n",
        "  # Select First Worksheet\n",
        "  ws = wb.worksheets[0]\n",
        "\n",
        "  for record in records:\n",
        "    # Append Row Values\n",
        "    ws.append(record)\n",
        "\n",
        "  wb.save(dir + \"tmp_results.xlsx\")\n",
        "\n",
        "def print_time(topic,start_time):\n",
        "  if get_times:\n",
        "    print(f\"    *** {topic} time: {time.time()-start_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EpUQKewJr3V"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "This is the original description of the dataset. Clearly, **our goal is to set up a completely different prediction task**.\n",
        "\n",
        "**Graph**: The ogbl-citation2 dataset is a directed graph, representing the citation network between a subset of papers extracted from MAG. Dach node is a paper with **128-dimensional word2vec features** that summarizes its title and abstract, and each directed edge indicates that **one paper cites another**. All nodes also come with meta-information indicating the year the corresponding paper was published.\n",
        "\n",
        "**Prediction task**: The task is to predict missing citations given existing citations. Specifically, for each source paper, two of its references are randomly dropped, and we would like the model to rank the missing two references higher than 1,000 negative reference candidates. The negative references are randomly-sampled from all the previous papers that are not referenced by the source paper. The evaluation metric is Mean Reciprocal Rank (MRR), where the reciprocal rank of the true reference among the negative candidates is calculated for each source paper, and then the average is taken over all source papers.\n",
        "\n",
        "**Dataset splitting**: We split the edges according to time, in order to simulate a realistic application in citation recommendation (e.g., a user is writing a new paper and has already cited several existing papers, but wants to be recommended additional references). To this end, we use the most recent papers (those published in 2019) as the source papers for which we want to recommend the references. For each source paper, we drop two papers from its references—the resulting two dropped edges (pointing from the source paper to the dropped papers) are used respectively for validation and testing. All the rest of the edges are used for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktfQQiDzJuUp"
      },
      "source": [
        "### Installation and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tOlunvuJtMs"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "################################################################################\n",
        "##### EXECUTE ONLY IF THE ORIGINAL GRAPH HAS TO BE DOWNLOADED (it takes time) ##\n",
        "from ogb.linkproppred import PygLinkPropPredDataset\n",
        "\n",
        "dataset = PygLinkPropPredDataset(name=graph_name)\n",
        "graph_0 = dataset[0]\n",
        "\n",
        "print(f'{graph_name} has {graph_0.num_nodes} nodes and {graph_0.num_edges} edges, with an average (incoming) node degree of {graph_0.num_edges / graph_0.num_nodes}')\n",
        "print(f'Each node has {len(graph_0.x[0])} features')\n",
        "\n",
        "# G = convert.to_networkx(graph_0, to_undirected=False)\n",
        "# Too big to be drawed\n",
        "# nx.draw(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzPAhw8a68x2"
      },
      "source": [
        "### Generating the reduced graph\n",
        "\n",
        "Once a reduced graph with a given number of nodes is generated, it is stored locally. Afterwards, the stored graph can be retrieved instead of having to generate it every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNNN3VpQLGVI",
        "outputId": "37b6fe15-78d5-49b6-e0ef-2c32fd0a4562"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-01c7b7b4d5ae>:30: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  x = graph.x[mask_n]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The reduced version of ogbl-citation2 has 80000 nodes and 1102930 edges, with an average (incoming) node degree of 13.786625\n",
            "Each node has 128 features\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "################################################################################\n",
        "##### EXECUTE ONLY IF THE ORIGINAL GRAPH HAS TO BE REDUCED #####################\n",
        "\n",
        "num_nodes_to_sample = 80000\n",
        "\n",
        "mask, graph = reduce_graph(graph=graph_0,\n",
        "                           n_nodes=num_nodes_to_sample,\n",
        "                           dense=True,\n",
        "                           reverse=False,\n",
        "                           clip=True,\n",
        "                           relabel_nodes=True)\n",
        "\n",
        "print(f'The reduced version of {graph_name} has {graph.num_nodes} nodes and {graph.num_edges} edges, with an average (incoming) node degree of {graph.num_edges / graph.num_nodes}')\n",
        "print(f'Each node has {len(graph.x[0])} features')\n",
        "\n",
        "torch.save(graph, dir + f\"reduced_graph_{num_nodes_to_sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fxYng-w79wd"
      },
      "source": [
        "### Loading the (previously generated) reduced graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ruFsrsVi8EYu"
      },
      "outputs": [],
      "source": [
        "def get_reduced_graph(sampled_nodes, node_features='original'):\n",
        "  graph = torch.load(dir + f\"reduced_graph_{sampled_nodes}\", map_location=torch.device(device)).to(device)\n",
        "\n",
        "  # Optionally, we can use newly-computed node features instead of the original 128-dimensional vector\n",
        "  # This choice leads to a clear information LEAKAGE: in some sense, features for test nodes would\n",
        "  # inherit information about the graph connectivity (that is, we are giving information that we are supposed to predict)\n",
        "  if node_features == 'stats':\n",
        "    print(\"Taking newly-generated node features instead of the original ones\")\n",
        "    G = utils.convert.to_networkx(graph, to_undirected=False)\n",
        "    # compute node stats using existing algorithms\n",
        "    pagerank = nx.algorithms.link_analysis.pagerank_alg.pagerank(G)\n",
        "    clustering_coef = nx.algorithms.cluster.clustering(G)\n",
        "    betweeness_centrality = nx.betweenness_centrality(G, k=50)\n",
        "    degree = G.degree()\n",
        "    # create initial node features from that\n",
        "    aug_emb = torch.ones(graph.num_nodes, 5, dtype=torch.float64).to(device)\n",
        "    for i in range(graph.num_nodes):\n",
        "      aug_emb[i][0] = degree[i]\n",
        "      aug_emb[i][1] = pagerank[i]\n",
        "      aug_emb[i][2] = betweeness_centrality[i]\n",
        "      aug_emb[i][3] = pagerank[i]\n",
        "      aug_emb[i][4] = 1.0\n",
        "      aug_emb = aug_emb.float()\n",
        "    graph = Data(x=aug_emb, edge_index=graph.edge_index).to(device)\n",
        "\n",
        "  if node_features == 'dummy': # TO-DO: to run tests with this choice gives some error; double-check\n",
        "    aug_emb = torch.ones(graph.num_nodes, 3, dtype=torch.float64).to(device)\n",
        "    aug_emb = aug_emb.float()\n",
        "    graph = Data(x=aug_emb, edge_index=graph.edge_index).to(device)\n",
        "\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNtlNZb6MWtn"
      },
      "source": [
        "### Other types of graphs\n",
        "\n",
        "Instead of the reduced version of ogbl-citation2, we can also load\n",
        "- A Barabási-Albert graph\n",
        "- An Erdös-Renyi random-generated graph\n",
        "- An egocentric network\n",
        "- etc.\n",
        "\n",
        "The idea is that these graphs have (if possible) the same number of nodes, and a similar number of edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sHOEXI0yMV6t"
      },
      "outputs": [],
      "source": [
        "def get_random_features(n_nodes,num_node_features):\n",
        "  # random values in [0,1]\n",
        "  x = torch.rand(n_nodes,num_node_features).to(device)\n",
        "  # map values to [-1,1]\n",
        "  x = (x-0.5)*2\n",
        "  return x\n",
        "\n",
        "def get_dummy_features(n_nodes,num_node_features=3):\n",
        "  x = torch.ones(n_nodes,num_node_features).to(device)\n",
        "  return x\n",
        "\n",
        "# We could use directly the PyG function instead of NetworkX\n",
        "def get_barabasi_albert_graph(n_nodes,n_edges,num_node_features):\n",
        "  x = get_random_features(n_nodes,num_node_features)\n",
        "\n",
        "  G = random_graphs.barabasi_albert_graph(n_nodes, round(n_edges/n_nodes/2))\n",
        "  graph = utils.convert.from_networkx(G).to(device)\n",
        "  graph.x = x\n",
        "  return graph\n",
        "\n",
        "def get_erdos_renyi_graph(n_nodes,n_edges,num_node_features):\n",
        "  x = get_random_features(n_nodes,num_node_features)\n",
        "\n",
        "  p = n_edges/(n_nodes*n_nodes)\n",
        "  # According to the documentation, this implementation is faster for sparse graphs\n",
        "  G = random_graphs.fast_gnp_random_graph(n_nodes,p)\n",
        "  #G = random_graphs.erdos_renyi_graph(n_nodes,p)\n",
        "  graph = utils.convert.from_networkx(G).to(device)\n",
        "  graph.x = x\n",
        "\n",
        "  return graph\n",
        "\n",
        "# Starting from the undirected version of the (big) citation graph, and one\n",
        "# randomly-chosen node, builds a connected graph with ~n_nodes\n",
        "# nodes, together with the original connections between them\n",
        "#\n",
        "# Needs the original graph 'graph_0' to have been previously loaded\n",
        "def get_ego_network_un(n_nodes):\n",
        "  # First, the undirected version of the original graph is computed\n",
        "  edge_index_un = utils.to_undirected(graph_0.edge_index)\n",
        "  # This indicates the minimum and maximum number of nodes allowed\n",
        "  min_ratio, max_ratio = [2/3,3/2]\n",
        "  final_nodes = sys.maxsize\n",
        "  # we keep generating ego networks randomly until a reasonable size is obtained (less than double the expected nodes)\n",
        "  while (final_nodes>n_nodes*max_ratio):\n",
        "    # First gets a randomly-chosen node\n",
        "    initial_node = random.randint(0,graph_0.num_nodes-1)\n",
        "    print(f\"Starting from node {initial_node}...\")\n",
        "    nodes = [initial_node]\n",
        "    k = 1\n",
        "    # Generates k_hop subgraph until the node number is enough\n",
        "    while len(nodes)<(n_nodes*min_ratio):\n",
        "      nodes, edge_index, mapping, edge_mask = utils.k_hop_subgraph([initial_node],\n",
        "                                                             k,\n",
        "                                                             edge_index_un,\n",
        "                                                             relabel_nodes=True)\n",
        "      print(f\"   k: {k} -> {len(nodes)} nodes\")\n",
        "      k = k+1\n",
        "    final_nodes = len(nodes)\n",
        "  # Filters node features\n",
        "  mask_n = torch.zeros(graph_0.num_nodes).byte()\n",
        "  for n in nodes:\n",
        "    mask_n[n] = True\n",
        "  x = graph_0.x[mask_n]\n",
        "  g = Data(x=x,edge_index=edge_index)\n",
        "  print(f\"Ego network generated with {g.num_nodes} nodes and {g.num_edges} edges\")\n",
        "  return g\n",
        "\n",
        "# This takes a reduced graph with N*train_ratio nodes, and adds the remaining\n",
        "# N*(1-train_ratio) nodes by using Barabási-Albert. The idea is that new nodes\n",
        "# are the test nodes\n",
        "def get_sampled_plus_barabasi_albert_graph(n_nodes,num_node_features):\n",
        "  n_train_nodes = round(n_nodes*train_ratio)\n",
        "  n_test_nodes = n_nodes-n_train_nodes\n",
        "\n",
        "  graph0 = get_reduced_graph(sampled_nodes=n_train_nodes, node_features='original')\n",
        "  G0 = utils.convert.to_networkx(graph0)\n",
        "  G = random_graphs.barabasi_albert_graph(n_nodes,round(1.5*graph0.num_edges/graph0.num_nodes),initial_graph=G0) # the second argument is meant to produce a similar number of edges as the dense subgraph\n",
        "\n",
        "  x = get_random_features(n_nodes,num_node_features)\n",
        "\n",
        "  graph = utils.convert.from_networkx(G).to(device)\n",
        "  graph.x = x\n",
        "\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5r83nHlZCGR"
      },
      "source": [
        "### Building data for training / test\n",
        "\n",
        "We are splitting nodes into **training** and **test**.\n",
        "\n",
        "Due to the nature of the problem, the training graph has a subset of nodes and the corresponding edges. Moreover, edges are **reversed** in order to be able to build proper **computations graphs**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gOWc2nwFKD4v"
      },
      "outputs": [],
      "source": [
        "def get_training_graph(graph):\n",
        "  # Computes the graph with only a percentage of nodes for training\n",
        "  train_mask, train_graph = reduce_graph(graph=graph,\n",
        "                                         n_nodes=round(graph.num_nodes*train_ratio),\n",
        "                                         dense=False,\n",
        "                                         reverse=True,\n",
        "                                         clip=False,\n",
        "                                         relabel_nodes=False)\n",
        "\n",
        "  nodes = torch.tensor(range(0,graph.num_nodes)).to(device)\n",
        "\n",
        "  train_nodes = nodes[train_mask]\n",
        "  test_mask = torch.logical_not(train_mask)\n",
        "  test_nodes = nodes[test_mask]\n",
        "\n",
        "  return train_graph, train_nodes, test_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMbIRgz-SlRo"
      },
      "source": [
        "### Loading the selected type of graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LzygFaLu1SxI"
      },
      "outputs": [],
      "source": [
        "def get_graph(graph_type,n_nodes,node_features_type):\n",
        "  path = dir + f\"reduced_graph_{n_nodes}\"\n",
        "  if os.path.exists(path):\n",
        "    graph0 = get_reduced_graph(n_nodes,node_features_type)\n",
        "    n_edges = graph0.num_edges\n",
        "    num_features = len(graph0.x[0])\n",
        "  else: # This happens if there is no subgraph with the specified number of nodes\n",
        "  # In this case, user-defined values for n_edges and num_features have to be provided\n",
        "    graph0 = None\n",
        "    n_edges = n_nodes*8 # put here a reasonable number wrt the number of nodes\n",
        "    num_features = 128\n",
        "    print(f\"WARNING: no graph to load with {n_nodes} nodes\")\n",
        "  if graph_type == 'sample':\n",
        "    graph = graph0\n",
        "    g_type = \"Sampled\"\n",
        "    train_graph, train_nodes, test_nodes = get_training_graph(graph)\n",
        "  elif graph_type == 'sample_undir':\n",
        "    graph = Data(x=graph0.x,edge_index=utils.to_undirected(graph0.edge_index))\n",
        "    g_type = \"Sampled (unidirected)\"\n",
        "    train_graph, train_nodes, test_nodes = get_training_graph(graph)\n",
        "  elif graph_type == 'ego': # Requires big graph to be loaded\n",
        "    graph = get_ego_network_un(n_nodes)\n",
        "    g_type = \"EGO\"\n",
        "    train_graph, train_nodes, test_nodes = get_training_graph(graph)\n",
        "  elif graph_type == 'ba':\n",
        "    graph = get_barabasi_albert_graph(n_nodes=n_nodes,n_edges=n_edges,num_node_features=num_features)\n",
        "    g_type = \"Barabási-Albert\"\n",
        "    train_graph, train_nodes, test_nodes = get_training_graph(graph)\n",
        "  elif graph_type == 'er':\n",
        "    graph = get_barabasi_albert_graph(n_nodes=n_nodes,n_edges=n_edges,num_node_features=num_features)\n",
        "    g_type = \"Erdös-Renyi\"\n",
        "    train_graph, train_nodes, test_nodes = get_training_graph(graph)\n",
        "  elif graph_type == 'sample+ba':\n",
        "    graph = get_sampled_plus_barabasi_albert_graph(n_nodes,num_features)\n",
        "    g_type = \"Sampled (+Barabási-Albert for test nodes)\"\n",
        "    n_train_nodes = round(n_nodes*train_ratio)\n",
        "    edges = subgraph(subset=list(range(n_train_nodes)),edge_index=graph.edge_index,relabel_nodes=False)\n",
        "    print(edges[0].shape)\n",
        "    train_graph = Data(x=graph.x,edge_index=edges[0])\n",
        "    nodes = torch.tensor(list(range(n_nodes)))\n",
        "    train_mask = torch.cat([torch.tensor([True]*n_train_nodes),torch.tensor([False]*(n_nodes-n_train_nodes))])\n",
        "    train_nodes = nodes[train_mask]\n",
        "    test_mask = torch.logical_not(train_mask)\n",
        "    test_nodes = nodes[test_mask]\n",
        "  else: # The \"else\" case is like sampled, but a warning message is output\n",
        "    graph = graph0\n",
        "    g_type = \"NOT SPECIFIED\"\n",
        "    train_graph, train_nodes, test_nodes = get_training_graph(graph)\n",
        "\n",
        "  print(f\"{g_type} graph with {graph.num_nodes} nodes and {graph.num_edges} edges loaded\")\n",
        "\n",
        "  print(f'The train graph has {len(train_nodes)} (actual) nodes and {train_graph.num_edges} edges')\n",
        "  print(f'Number of test nodes: {len(test_nodes)}')\n",
        "\n",
        "  return graph.to(device), train_graph.to(device), train_nodes.to(device), test_nodes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICPj_-EI-xKu"
      },
      "source": [
        "## Neural Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhHuhYs1Yw0n"
      },
      "source": [
        "### GNN architectures\n",
        "\n",
        "We have GCN, GAT, ClusterGCN and SAGE available.\n",
        "All architectures are a sequence of GNN layers of a given class, with ReLU between layers, and a configurable dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NuC4hpOQTphW"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "    for _ in range(num_layers - 2):\n",
        "      self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "    self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "\n",
        "    self.norm = torch.nn.ModuleList()\n",
        "    for _ in range(num_layers-1):\n",
        "      self.norm.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    for conv,norm in zip(self.convs[:-1],self.norm):\n",
        "      x = conv(x, edge_index)\n",
        "      x = F.relu(x)\n",
        "      x = norm(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, edge_index)\n",
        "    return x\n",
        "\n",
        "class ClusterGCN(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.convs.append(ClusterGCNConv(in_channels, hidden_channels))\n",
        "    for _ in range(num_layers - 2):\n",
        "      self.convs.append(ClusterGCNConv(hidden_channels, hidden_channels))\n",
        "    self.convs.append(ClusterGCNConv(hidden_channels, out_channels))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    for conv in self.convs[:-1]:\n",
        "      x = conv(x, edge_index)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, edge_index)\n",
        "    return x\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.convs.append(GATConv(in_channels, hidden_channels))\n",
        "    for _ in range(num_layers - 2):\n",
        "      self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
        "    self.convs.append(GATConv(hidden_channels, out_channels))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    for conv in self.convs[:-1]:\n",
        "      x = conv(x, edge_index)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, edge_index)\n",
        "    return x\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, aggr=\"add\"):\n",
        "    super(SAGE, self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.convs.append(SAGEConv(in_channels, hidden_channels, normalize=True, aggr=aggr))\n",
        "    for _ in range(num_layers - 2):\n",
        "      self.convs.append(SAGEConv(hidden_channels, hidden_channels, normalize=True, aggr=aggr))\n",
        "    self.convs.append(SAGEConv(hidden_channels, out_channels, normalize=True, aggr=aggr))\n",
        "\n",
        "    self.norm = torch.nn.ModuleList()\n",
        "    for _ in range(num_layers-1):\n",
        "      self.norm.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    for conv,norm in zip(self.convs[:-1],self.norm):\n",
        "      x = conv(x, edge_index)\n",
        "      x = F.relu(x)\n",
        "      x = norm(x)\n",
        "      running_mean = torch.mean(x, -1) # it can't go into the batch_norm function as a parameter\n",
        "      running_var = torch.var(x, -1) # it can't go into the batch_norm function as a parameter\n",
        "      # x = torch.nn.functional.batch_norm(x, running_mean, running_var)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, edge_index)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv0_drtM4NQf"
      },
      "source": [
        "### Architecture for **predictors**\n",
        "\n",
        "MLPNodePredictor\n",
        "  - takes an example (sources,target)\n",
        "  - computes an embedding **e_sources** that is the point-wise average of the embeddings corresponding to sources\n",
        "  - takes the embedding **e_target** corresponding to target\n",
        "  - concatenates **e_sources** with **e_target**\n",
        "  - applies a DL arquitecture to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3W7dqw8RWKGH"
      },
      "outputs": [],
      "source": [
        "class NodePredictor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NodePredictor, self).__init__()\n",
        "\n",
        "  def forward(self, examples, node_embs):\n",
        "    pass\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    pass\n",
        "\n",
        "class MLPNodePredictor(NodePredictor):\n",
        "  def __init__(self,embedding_size):\n",
        "    super(MLPNodePredictor, self).__init__()\n",
        "\n",
        "    # These numbers should better be powers of 2\n",
        "    hidden_dimension1 = round(embedding_size/2)\n",
        "    hidden_dimension2 = round(hidden_dimension1/4)\n",
        "\n",
        "    self.lin1 = torch.nn.Linear(2*embedding_size, hidden_dimension1)\n",
        "    self.lin2 = torch.nn.Linear(hidden_dimension1, hidden_dimension2)\n",
        "    self.lin3 = torch.nn.Linear(hidden_dimension2, 1)\n",
        "\n",
        "  def forward(self,examples,node_embs):\n",
        "    (sources,targets) = examples\n",
        "\n",
        "    # Computing embeddings for examples\n",
        "    # This has to be done each time because the last-generated node embeddings\n",
        "    # have to be used\n",
        "    sources_embs = torch.stack([node_embs[source.t()].mean(0) for source in sources])\n",
        "    sources_embs = torch.nan_to_num(sources_embs)\n",
        "    target_embs = torch.stack([node_embs[target] for target in targets])\n",
        "    x = torch.cat([sources_embs,target_embs], 1)\n",
        "\n",
        "    x = self.lin1(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.lin2(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.lin3(x)\n",
        "    out = torch.flatten(x)\n",
        "\n",
        "    return torch.sigmoid(out)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    self.lin1.reset_parameters()\n",
        "    self.lin2.reset_parameters()\n",
        "    self.lin3.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esCgEqwcDpvr"
      },
      "source": [
        "## Example generation\n",
        "\n",
        "This code is in charge of generating **positive** and **negative** examples, for both **training** and **test**.\n",
        "\n",
        "What distinguishes a training example from a test one is the set of nodes the **target node** is taken from: either the train or the test portion of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z9nxwKtQPb3U"
      },
      "outputs": [],
      "source": [
        "# This function takes a set of nodes and returns a randomly generated subset of\n",
        "# it such that the ratio of selected nodes is between 'at_least' and 'at_most'\n",
        "# (both taking values between 0 and 1)\n",
        "def filter_nodes(nodes,at_least,at_most):\n",
        "  if at_least>at_most: # this should never happen\n",
        "    at_most = at_least\n",
        "\n",
        "  l = len(nodes)\n",
        "  # nodes are first shuffled\n",
        "  rand_indx = torch.randperm(l)\n",
        "  nodes = nodes[rand_indx]\n",
        "\n",
        "  mask = torch.rand(l)\n",
        "  mask = mask>0.5\n",
        "  for i in range(l):\n",
        "    if i<l*at_least: # nodes that DO have to be included\n",
        "      mask[i] = True\n",
        "    elif i>=l*at_most: # nodes that DO NOT have to be included\n",
        "      mask[i] = False\n",
        "\n",
        "  return nodes[mask]\n",
        "\n",
        "# This function generates a list of examples of type T with T in {pos,neg}.\n",
        "# This means that it comes up with randomly chosen pairs (S,t) such that\n",
        "# - S is a set of nodes\n",
        "# - t is a node\n",
        "# - if T is 'pos': a minimal proportion 'min_pure' of nodes s such that (s,t)\n",
        "#   is in the graph (a positive edge) is in S, and a maximal proportion\n",
        "#   'max_spurious' of nodes s such that (s,t) is not in the graph (a negative\n",
        "#   edge) is in S\n",
        "# - is T is 'neg': the other way around\n",
        "def generate_examples(type,total_nodes,target_nodes,train_sample_ratio,pos_edges,neg_edges,min_pure=1.0,max_spurious=0.0):\n",
        "  sources = torch.zeros(len(target_nodes),total_nodes).bool().to(device)\n",
        "  targets = torch.zeros(len(target_nodes)).int().to(device)\n",
        "\n",
        "  n = 0\n",
        "  rand_sample = torch.randperm(len(target_nodes))[:(round(train_sample_ratio*len(target_nodes)))]\n",
        "\n",
        "  for target in target_nodes[rand_sample]:\n",
        "    # pick randomly a node target with incoming edges\n",
        "    #target = target_nodes[random.randint(0,len(target_nodes)-1)]\n",
        "    # take nodes with POSITIVE outcoming edges pointing to target\n",
        "    pos_sources = pos_edges[0,pos_edges[1,:]==target]\n",
        "    # take nodes with NEGATIVE outcoming edges pointing to target\n",
        "    neg_sources = neg_edges[0,neg_edges[1,:]==target]\n",
        "\n",
        "    if type == 'pos':\n",
        "      # positive edges are considered as pure\n",
        "      filtered_pos_sources = filter_nodes(pos_sources,min_pure,1.00)\n",
        "      # negative edges are considered as spurious\n",
        "      filtered_neg_sources = filter_nodes(neg_sources,0.00,max_spurious)\n",
        "    else:\n",
        "      # positive edges are considered as spurious\n",
        "      filtered_pos_sources = filter_nodes(pos_sources,0.00,max_spurious)\n",
        "      # negative edges are considered as pure\n",
        "      filtered_neg_sources = filter_nodes(neg_sources,min_pure,1.00)\n",
        "\n",
        "    # this loop is for debug purposes only, when torch_scatter is nos imported\n",
        "    #for x in torch.cat([filtered_pos_sources,filtered_neg_sources]):\n",
        "    #  sources[n][x] = True\n",
        "    sources[n].scatter_(dim=0, index=torch.cat([filtered_pos_sources,filtered_neg_sources]), value=True)\n",
        "    targets[n] = int(target)\n",
        "\n",
        "    n = n+1\n",
        "\n",
        "  return (sources,targets)\n",
        "\n",
        "# This function is supposed to generate labeled positive and negative examples\n",
        "# related to target nodes that are in a random set nodes of nodes.\n",
        "#\n",
        "# One positive and one negative example is generated for each target node\n",
        "def create_labeled_examples(total_nodes,target_nodes,edge_index,neg_edges,purity,train_sample_ratio=1.0):\n",
        "  n_pos_examples = n_neg_examples = round(train_sample_ratio*len(target_nodes))\n",
        "  # Positive examples are generated such as they have a node from 'nodes' as their target\n",
        "  (pos_sources,pos_targets) = generate_examples('pos',\n",
        "                                                total_nodes=total_nodes,\n",
        "                                                target_nodes=target_nodes,\n",
        "                                                train_sample_ratio=train_sample_ratio,\n",
        "                                                pos_edges=edge_index,\n",
        "                                                neg_edges=neg_edges,\n",
        "                                                min_pure = purity['min_pure'],\n",
        "                                                max_spurious = purity['max_spurious'])\n",
        "  pos_labels = torch.ones(n_pos_examples).to(device)\n",
        "  # Negative examples are generated such as they have a node from 'nodes' as their target\n",
        "  (neg_sources,neg_targets) = generate_examples('neg',\n",
        "                                                total_nodes=total_nodes,\n",
        "                                                target_nodes=target_nodes,\n",
        "                                                train_sample_ratio=train_sample_ratio,\n",
        "                                                pos_edges=edge_index,\n",
        "                                                neg_edges=neg_edges,\n",
        "                                                min_pure = purity['min_pure'],\n",
        "                                                max_spurious = purity['max_spurious'])\n",
        "  neg_labels = torch.zeros(n_neg_examples).to(device)\n",
        "\n",
        "  # Joining positive and negative examples and shuffling\n",
        "  train_sources = torch.cat([pos_sources,neg_sources]).to(device)\n",
        "  train_targets = torch.cat([pos_targets,neg_targets]).to(device)\n",
        "  train_labels = torch.cat([pos_labels,neg_labels]).to(device)\n",
        "  rp = torch.randperm(n_pos_examples+n_neg_examples)\n",
        "  return (train_sources[rp],train_targets[rp]), train_labels[rp]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadkHHXYW-Pd"
      },
      "source": [
        "## Training driver\n",
        "\n",
        "The GNN runs on the train part of the graph (a complete subgraph with a portion of nodes, and the corresponding edges between them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9zGQIV08W90k"
      },
      "outputs": [],
      "source": [
        "# Patience is the number of epochs after which we want to stop on no improvements\n",
        "def early_stopping(epoch,model,predictor,h,val_examples,val_labels,val_loss_history,best_val_loss,patience):\n",
        "  val_preds = predictor(val_examples,h)\n",
        "  val_correct_predictions = len([n for n in val_preds[val_labels==1.0] if n>=0.5]) + len([n for n in val_preds[val_labels==0.0] if n<0.5])\n",
        "  val_loss = loss_fn(val_preds, val_labels)\n",
        "  val_loss_history.append(val_loss.item())\n",
        "  print(f\"Computing validation loss: {val_loss.item():.4f}\")\n",
        "  if val_loss<best_val_loss:\n",
        "    print(f\"Saving best model (epoch {epoch})\")\n",
        "    torch.save(model.state_dict(),dir + \"best_model.pth\")\n",
        "    torch.save(predictor.state_dict(),dir+ \"best_predictor.pth\")\n",
        "    best_val_loss = val_loss\n",
        "  if len(val_loss_history)>patience:\n",
        "    x = val_loss_history[-(patience+1)]\n",
        "    es = True\n",
        "    for i in range(patience):\n",
        "      es = es & (x<=val_loss_history[-(patience-i)])\n",
        "    if es:\n",
        "      print(f\"Val loss {x:.4f} at epoch {len(val_loss_history)-patience} better than {val_loss_history[-patience:]}\")\n",
        "  else:\n",
        "    es = False\n",
        "  return (es,best_val_loss)\n",
        "\n",
        "def train(model,predictor,train_graph,train_nodes,train_sample_ratio,loss_fn,optimizer,batch_size,num_epochs,purity):\n",
        "  check_early_stopping = True\n",
        "\n",
        "  # Relative size of the validation set (only used with early stopping)\n",
        "  val_size = 0.1 if check_early_stopping else 0.0\n",
        "  best_val_loss = float('inf')\n",
        "\n",
        "  model.train()\n",
        "  predictor.train()\n",
        "\n",
        "  # Generating negative edges (once and for all)\n",
        "  neg_edges = utils.negative_sampling(train_graph.edge_index, num_nodes=len(train_graph.x),\n",
        "                                num_neg_samples=train_graph.edge_index.shape[1], method='dense').to(device)\n",
        "  # At training time, negative edges have to be restricted to the train graph\n",
        "  # Due to this, and to how negative edges are created, the number of negative\n",
        "  # edges comes to be a bit smaller than positive edges.\n",
        "  neg_edges = subgraph(subset=train_nodes,edge_index=neg_edges,relabel_nodes=False)[0]\n",
        "\n",
        "  # Keeping track of loss history, together with the best epoch for loss and accuracy\n",
        "  loss_history = []\n",
        "  val_loss_history = []\n",
        "  # To be returned later\n",
        "  # Minimum loss\n",
        "  min_loss = float('inf')\n",
        "  # Epoch at which the minimum loss is reached\n",
        "  min_loss_epoch = 0\n",
        "  # Maximum accuracy\n",
        "  max_acc = 0.0\n",
        "  # Epoch at which the maximum accuracy is reached\n",
        "  max_acc_epoch = 0\n",
        "\n",
        "  n_pos_examples = n_neg_examples = round(train_sample_ratio*len(train_nodes))\n",
        "\n",
        "  start_time = time.time() if get_times else 0\n",
        "  print(f\"Generating {n_pos_examples}+{n_neg_examples} total examples\")\n",
        "  (sources,targets), labels = create_labeled_examples(total_nodes=train_graph.num_nodes,\n",
        "                                                      target_nodes=train_nodes,\n",
        "                                                      train_sample_ratio=train_sample_ratio,\n",
        "                                                      edge_index=train_graph.edge_index,\n",
        "                                                      neg_edges=neg_edges,\n",
        "                                                      purity=purity)\n",
        "\n",
        "  # If needed, splitting into train and validation examples\n",
        "  limit = round(len(labels)*(1-val_size))\n",
        "  train_sources = sources[:limit]\n",
        "  train_targets = targets[:limit]\n",
        "  train_examples = (train_sources,train_targets)\n",
        "  train_labels = labels[:limit]\n",
        "  val_sources = sources[limit:]\n",
        "  val_targets = targets[limit:]\n",
        "  val_examples = (val_sources,val_targets)\n",
        "  val_labels = labels[limit:]\n",
        "  print(f\"{labels.shape[0]} examples generated: {train_labels.shape[0]} train examples + {val_labels.shape[0]} val examples\")\n",
        "  print_time(\"Example-generation\",start_time)\n",
        "\n",
        "  print(f\"Positive labels: {train_labels.sum(-1)}\")\n",
        "\n",
        "  for epoch in range(1,num_epochs+1):\n",
        "    epoch_start_time = time.time() if get_times else 0\n",
        "    epoch_correct_predictions = 0\n",
        "    epoch_total_predictions = 0\n",
        "    epoch_total_loss = 0\n",
        "\n",
        "    for perm in DataLoader(range(train_examples[0].shape[0]), batch_size, shuffle=True):\n",
        "      batch_start_time = time.time() if get_times else 0\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_examples = (train_sources[perm],train_targets[perm])\n",
        "      batch_labels = train_labels[perm]\n",
        "\n",
        "      # Using the GNN to generate node embeddings\n",
        "      # Embeddings are computed for ALL the training graph every time\n",
        "      start_time = time.time() if get_times else 0\n",
        "      h = model(train_graph.x, train_graph.edge_index)\n",
        "      print_time(\"Node-embeddings\",start_time)\n",
        "\n",
        "      # Getting predictions for our batch, and computing the training loss\n",
        "      start_time = time.time() if get_times else 0\n",
        "      preds = predictor(batch_examples,h)\n",
        "      epoch_correct_predictions += len([n for n in preds[batch_labels==1.0] if n>=0.5]) + len([n for n in preds[batch_labels==0.0] if n<0.5])\n",
        "      epoch_total_predictions += preds.shape[0]\n",
        "      loss = loss_fn(preds, batch_labels)\n",
        "      epoch_total_loss += loss.item()\n",
        "      print_time(\"Prediction\",start_time)\n",
        "\n",
        "      # Updating our parameters\n",
        "      start_time = time.time() if get_times else 0\n",
        "      loss.backward()\n",
        "      print_time(\"Backpropagation\",start_time)\n",
        "      start_time = time.time() if get_times else 0\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "      print_time(\"Optimization\",start_time)\n",
        "      print_time(\"Batch\",batch_start_time)\n",
        "\n",
        "    epoch_acc = 100*epoch_correct_predictions/epoch_total_predictions\n",
        "\n",
        "    # Updating loss and accuracy history\n",
        "    loss_history.append(epoch_total_loss)\n",
        "    if epoch_total_loss < min_loss:\n",
        "      min_loss = epoch_total_loss\n",
        "      min_loss_epoch = epoch\n",
        "    if epoch_acc > max_acc:\n",
        "      max_acc = epoch_acc\n",
        "      max_acc_epoch = epoch\n",
        "\n",
        "    if check_early_stopping:\n",
        "      patience = 8\n",
        "      es,bvs = early_stopping(epoch=epoch,\n",
        "                              model=model,\n",
        "                              predictor=predictor,\n",
        "                              h=h,\n",
        "                              val_examples=val_examples,\n",
        "                              val_labels=val_labels,\n",
        "                              val_loss_history=val_loss_history,\n",
        "                              best_val_loss=best_val_loss,\n",
        "                              patience=patience)\n",
        "      best_val_loss = bvs # updating best_val_loss\n",
        "      if es:\n",
        "        print(f'EARLY STOPPING AT EPOCH {epoch}; best model: epoch {epoch-patience}')\n",
        "        break\n",
        "\n",
        "    torch.save(model.state_dict(),dir + f\"model_{epoch}.pth\")\n",
        "    torch.save(predictor.state_dict(),dir+ f\"predictor_{epoch}.pth\")\n",
        "\n",
        "    print(f'Epoch {(epoch):4d} has loss {round(epoch_total_loss, 4):.4f}; ' +\n",
        "          f'correct predictions: {epoch_correct_predictions:6d} out of {epoch_total_predictions:6d} ' +\n",
        "          f'({epoch_acc:3.4f}%)')\n",
        "\n",
        "    print_time(\"Epoch\",epoch_start_time)\n",
        "\n",
        "  return min_loss, min_loss_epoch, max_acc, max_acc_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyZ8qLMjoBiH"
      },
      "source": [
        "## Test driver\n",
        "\n",
        "This code\n",
        "- generates embeddings for previously unseen nodes (not belonging to the train graph)\n",
        "- generates positive and negative examples with those nodes as target\n",
        "- classifies the examples\n",
        "- returns several metrics: accuracy, Hits@K, MRR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "A2hb7pvZnztp"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(loss,test_correct_predictions,test_total_predictions):\n",
        "  acc = 100*test_correct_predictions/test_total_predictions\n",
        "  return acc\n",
        "\n",
        "def get_hits_at_k(k, test_examples, test_labels, preds):\n",
        "  pos_examples_preds = preds[test_labels==1.0]\n",
        "  n_pos = len(pos_examples_preds)\n",
        "  neg_examples_preds = preds[test_labels==0.0]\n",
        "  n_neg = len(neg_examples_preds)\n",
        "\n",
        "  hits = torch.tensor([len((neg_examples_preds>p).nonzero()) for p in pos_examples_preds])\n",
        "  h = len((hits<k).nonzero())/n_pos\n",
        "  return h\n",
        "\n",
        "def get_mrr(test_examples, test_labels, preds):\n",
        "  pos_examples_preds = preds[test_labels==1.0]\n",
        "  n_pos = len(pos_examples_preds)\n",
        "  neg_examples_preds = preds[test_labels==0.0]\n",
        "  n_neg = len(neg_examples_preds)\n",
        "\n",
        "  ranks = torch.tensor([len((neg_examples_preds>p).nonzero())+1 for p in pos_examples_preds])\n",
        "  ranks_reciprocal = torch.reciprocal(ranks)\n",
        "\n",
        "  mrr = (ranks_reciprocal.sum())/n_pos\n",
        "  return mrr.item()\n",
        "\n",
        "def test(model,predictor,test_graph,test_nodes,loss_fn,num_test_examples,purity):\n",
        "  num_nodes = len(test_graph.x)\n",
        "\n",
        "  # Generating negative edges (once and for all)\n",
        "  neg_edges = utils.negative_sampling(test_graph.edge_index, num_nodes=num_nodes,\n",
        "                                num_neg_samples=test_graph.edge_index.shape[1], method='dense').to(device)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  test_examples, test_labels = create_labeled_examples(total_nodes=test_graph.num_nodes,\n",
        "                                                       target_nodes=test_nodes,\n",
        "                                                       edge_index=test_graph.edge_index,\n",
        "                                                       neg_edges=neg_edges,\n",
        "                                                       purity=purity)\n",
        "\n",
        "  # Use the GNN to generate node embeddings\n",
        "  h = model(test_graph.x, test_graph.edge_index)\n",
        "  # print(f'Model output: {h} (shape {h.shape})')\n",
        "\n",
        "  # Get predictions and compute the loss\n",
        "  preds = predictor(test_examples,h)\n",
        "  test_correct_predictions = len([n for n in preds[test_labels==1.0] if n>=0.5]) + len([n for n in preds[test_labels==0.0] if n<0.5])\n",
        "  test_total_predictions = preds.shape[0]\n",
        "\n",
        "  loss = loss_fn(preds, test_labels)\n",
        "\n",
        "  # Printing results\n",
        "  accuracy = get_accuracy(loss,test_correct_predictions,test_total_predictions)\n",
        "  print(f'Test loss: {round(loss.item(), 4)}; ' +\n",
        "        f'Accuracy (correct predictions): {test_correct_predictions} out of {test_total_predictions} ' +\n",
        "        f'({accuracy:.4f}%)')\n",
        "  hits = {}\n",
        "  num_neg_examples = len(preds[test_labels==0.0])\n",
        "  print(f\"Hits@k (wrt {num_neg_examples} negative examples): \", end=\"\")\n",
        "  for k in [1,2,3,5,10,20,30,50]:\n",
        "    hits[k] = get_hits_at_k(k, test_examples, test_labels, preds)\n",
        "    print(f\"k={k}: {hits[k]:.4f}; \", end=\"\")\n",
        "  print()\n",
        "  mrr = get_mrr(test_examples, test_labels, preds)\n",
        "  print(f\"MRR: {mrr:.4f}\")\n",
        "\n",
        "  return accuracy, hits, mrr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMoSu6pH-Lst"
      },
      "source": [
        "## Training and testing the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOR6fdQf-T3-"
      },
      "source": [
        "### Run configurations\n",
        "\n",
        "The **options** dictionary assigns values for different parameters.\n",
        "The following cell runs tests corresponding to such parameters.\n",
        "\n",
        "For each parameter, a list of values can be specified; if the list contains more than one value, then differnt test will be run.\n",
        "\n",
        "Ex. `options['sampled_nodes'] = [3000,10000]` runs experiments on the 3000-nodes and 10000-nodes of a graph.\n",
        "\n",
        "Moreover, it is possible to specify the value(s) of a parameter as dependent on some other parameter: for example, tha size of a batch may depend on the size of the graph. This can be done by assigning a dictionary, instead of a list, to a parameter.\n",
        "\n",
        "Ex. `options['batch_size'] = { 3000: [512], 10000: [2048], 20000: [4096], 50000: [8192] }` specifies a different value for the batch size depending on the size of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWPHV64p-kEU",
        "outputId": "1f39d18c-fcd3-41d9-c343-88ee635a6e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A total number of 1 learning tasks will be executed 1 times\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "\n",
        "# The type of graph\n",
        "options['graph_type'] = ['sample_undir'] #['sample','sample_undir','ego','ba','er','sample+ba']\n",
        "# The number of total nodes (in some graph types, like ego, the number of final node is normally bigger)\n",
        "options['sample_nodes'] = [3000]\n",
        "# Whether we use the original 128-dimensional vector of node features, or we compute stats-related ones, or we use dummy ones [1]\n",
        "options['node_features'] = ['original'] #['original','stats','dummy']\n",
        "# Type of GNN architecture\n",
        "options['GNN'] = [SAGE]\n",
        "# Number of layers (it should probably depend on 'sampled_nodes' and/or 'GNN')\n",
        "options['num_layers'] = { GCN: [5], ClusterGCN: [5], GAT: [5], SAGE: [5] }\n",
        "# Dropout coefficient (it should probably depend on 'GNN' and/or 'num_layers')\n",
        "options['dropout'] = { GCN: [0.2], ClusterGCN: [0.2], GAT: [0.2], SAGE: [0.2] }\n",
        "# Size of the embeddings vector computed by the GNN\n",
        "options['embedding_size'] = { GCN: [128], ClusterGCN: [128], GAT: [128], SAGE: [128] }\n",
        "# degree of purity (how many pure edges has to be included in an example, and how many spurious edges are allowed)\n",
        "# Let G be the graph under study, and G- be the negative graph (i.e., the graph\n",
        "# with the same nodes that is made of all the negative edges).\n",
        "\n",
        "# Given an example x=(S,t), that can be a positive or negative one, an edge e is\n",
        "# pure for x if both e and x have the same polarity (i.e., both positive or both\n",
        "# negative); otherwise, it is spurious\n",
        "\n",
        "# This parameter indicates how much \"impurity\" is allowed in an example:\n",
        "# Let t be the target node of the example; then, let S be the set of nodes in G\n",
        "# such that, for every s in S, the edge (s,t) is in G; let also S- be the set of\n",
        "# nodes in G- such that, for every s in S, the edge (s,t) is in G-. In other\n",
        "# words, given t, S (resp., S-) are the nodes connected to t by positive (resp.,\n",
        "# negative) edges.\n",
        "# The parameter min_pure indicates either\n",
        "# - the MINIMUM proportion of S that has to be included in a positive example on t; or\n",
        "# - the MINIMUM proportion of S- that has to be included in a negative example on t\n",
        "# The parameter max_spurious indicates either\n",
        "# - the MAXIMUM proportion of S that has to be included in a negative example on t; or\n",
        "# - the MAXIMUM proportion of S- that has to be included in a positive example on t\n",
        "#\n",
        "# For example, if we are considering a positive example, S contains 20 nodes, S-\n",
        "# contains 10 nodes, min_pure=0.8, max_spurious=0.2: the example will contain\n",
        "# AT LEAST 16=20*0.8 positive edges and AT MOST 2=10*0.2 negative edges\n",
        "#\n",
        "# Purity may be different at train and test time\n",
        "options['train_purity'] = [{'min_pure': 0.8,'max_spurious': 0.1}] # [{'min_pure': 1.0, 'max_spurious': 0.0},{'min_pure': 0.8,'max_spurious': 0.1},{'min_pure': 0.5,'max_spurious': 0.2}]\n",
        "# In general, one positive and one negative example is generated for every target node at training time; however, it is possible to specify a ratio of nodes for which examples are generated\n",
        "options['train_sample_ratio'] = [1.0]#,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
        "options['learning_rate'] = { GCN: [0.001,0.01,0.05], ClusterGCN: [0.001,0.01,0.05], GAT: [0.001,0.01,0.05], SAGE: [0.001] }\n",
        "options['test_purity'] = [{'min_pure': 0.8,'max_spurious': 0.1}] # [{'min_pure': 1.0, 'max_spurious': 0.0},{'min_pure': 0.8,'max_spurious': 0.1},{'min_pure': 0.5,'max_spurious': 0.2}]\n",
        "# the size of each training batch (it depends on 'sampled_nodes')\n",
        "options['batch_size'] = { 3000: [512], 10000: [2048], 20000: [4096], 50000: [8192] }\n",
        "# the number of training epochs\n",
        "options['num_epochs'] = [5]\n",
        "\n",
        "number_of_runs = 1 # This is used to run the same test several times\n",
        "\n",
        "n = 0\n",
        "for graph_type in options['graph_type']:\n",
        "  for sample_nodes in options['sample_nodes']:\n",
        "    for node_features in options['node_features']:\n",
        "      for gnn in options['GNN']:\n",
        "        for num_layers in options['num_layers'][gnn]:\n",
        "          for embedding_size in options['embedding_size'][gnn]:\n",
        "            for dropout in options['dropout'][gnn]:\n",
        "              for batch_size in options['batch_size'][sample_nodes]:\n",
        "                for num_epochs in options[\"num_epochs\"]:\n",
        "                  for train_purity in options['train_purity']:\n",
        "                    for train_sample_ratio in options['train_sample_ratio']:\n",
        "                      for learning_rate in options['learning_rate'][gnn]:\n",
        "                        for test_purity in options['test_purity']:\n",
        "                          n = n+1\n",
        "print(f'A total number of {n} learning tasks will be executed {number_of_runs} times')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLzfOv1X-Sj_"
      },
      "source": [
        "## Executing the battery of runs\n",
        "\n",
        "Experiments are run based on the values specified in the options dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrkmAuLAFipX",
        "outputId": "d7345e56-4ac8-4038-8e90-64560cd84b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### DATA\n",
            "Sampled (unidirected) graph with 3000 nodes and 31860 edges loaded\n",
            "The train graph has 2400 (actual) nodes and 19774 edges\n",
            "Number of test nodes: 600\n",
            "### MODEL: SAGE with 5 layers, 128 inputs, 128 hidden dimensions, and dropout 0.2\n",
            "SAGE(\n",
            "  (convs): ModuleList(\n",
            "    (0-4): 5 x SAGEConv(128, 128, aggr=add)\n",
            "  )\n",
            "  (norm): ModuleList(\n",
            "    (0-3): 4 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "MLPNodePredictor(\n",
            "  (lin1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (lin2): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (lin3): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "### TRAINING with 2400+2400 (100.000000% sample) train/val examples, purity (0.8, 0.1), batch_size = 512, num_epochs = 5, learning rate = 0.001\n",
            "Generating 2400+2400 total examples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-3ef3754bdbda>:12: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  train_nodes = nodes[train_mask]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4800 examples generated: 4320 train examples + 480 val examples\n",
            "Positive labels: 2166.0\n",
            "Computing validation loss: 0.6822\n",
            "Saving best model (epoch 1)\n",
            "Epoch    1 has loss 6.1998; correct predictions:   2789 out of   4320 (64.5602%)\n",
            "Computing validation loss: 0.6663\n",
            "Saving best model (epoch 2)\n",
            "Epoch    2 has loss 6.0806; correct predictions:   3611 out of   4320 (83.5880%)\n",
            "Computing validation loss: 0.6422\n",
            "Saving best model (epoch 3)\n",
            "Epoch    3 has loss 5.9193; correct predictions:   3599 out of   4320 (83.3102%)\n",
            "Computing validation loss: 0.6067\n",
            "Saving best model (epoch 4)\n",
            "Epoch    4 has loss 5.6539; correct predictions:   3604 out of   4320 (83.4259%)\n",
            "Computing validation loss: 0.5629\n",
            "Saving best model (epoch 5)\n",
            "Epoch    5 has loss 5.3170; correct predictions:   3590 out of   4320 (83.1019%)\n",
            "Minimum loss:     5.3170 reached at epoch 5\n",
            "Maximum accuracy: 83.5880 reached at epoch 2\n",
            "Training time: 11.686277151107788 s\n",
            "\n",
            "### TEST (x5) with 600+600 examples and purity (0.8, 0.1)\n",
            "Test loss: 0.5573; Accuracy (correct predictions): 1044 out of 1200 (87.0000%)\n",
            "Hits@k (wrt 600 negative examples): k=1: 0.0850; k=2: 0.2133; k=3: 0.2650; k=5: 0.4067; k=10: 0.6650; k=20: 0.7467; k=30: 0.7817; k=50: 0.8183; \n",
            "MRR: 0.2424\n",
            "Test loss: 0.5542; Accuracy (correct predictions): 1046 out of 1200 (87.1667%)\n",
            "Hits@k (wrt 600 negative examples): k=1: 0.2983; k=2: 0.3233; k=3: 0.3233; k=5: 0.3600; k=10: 0.7217; k=20: 0.7567; k=30: 0.7783; k=50: 0.8167; \n",
            "MRR: 0.3769\n",
            "Test loss: 0.5523; Accuracy (correct predictions): 1054 out of 1200 (87.8333%)\n",
            "Hits@k (wrt 600 negative examples): k=1: 0.3200; k=2: 0.3550; k=3: 0.5167; k=5: 0.6417; k=10: 0.7450; k=20: 0.7867; k=30: 0.8067; k=50: 0.8500; \n",
            "MRR: 0.4412\n",
            "Test loss: 0.5561; Accuracy (correct predictions): 1036 out of 1200 (86.3333%)\n",
            "Hits@k (wrt 600 negative examples): k=1: 0.0067; k=2: 0.0650; k=3: 0.1650; k=5: 0.6317; k=10: 0.7133; k=20: 0.7617; k=30: 0.7733; k=50: 0.8050; \n",
            "MRR: 0.1899\n",
            "Test loss: 0.5563; Accuracy (correct predictions): 1046 out of 1200 (87.1667%)\n",
            "Hits@k (wrt 600 negative examples): k=1: 0.3500; k=2: 0.4650; k=3: 0.5483; k=5: 0.6267; k=10: 0.7183; k=20: 0.7617; k=30: 0.7833; k=50: 0.8333; \n",
            "MRR: 0.4698\n",
            "\n",
            "Average values (on 5 test runs)\n",
            "- Accuracy (correct predictions): 87.1000%\n",
            "- Hits@k (wrt 600 negative examples): k=1: 0.2120; k=2: 0.2843; k=3: 0.3637; k=5: 0.5333; k=10: 0.7127; k=20: 0.7627; k=30: 0.7847; k=50: 0.8247; \n",
            "- MRR: 0.3440\n",
            "### Learning task #1 done\n",
            "\n",
            "\n",
            "Executed 1 learning tasks\n"
          ]
        }
      ],
      "source": [
        "n = 0\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "this_run = 0\n",
        "while (this_run<number_of_runs):\n",
        "  for graph_type in options['graph_type']:\n",
        "    for sample_nodes in options['sample_nodes']:\n",
        "      for node_features in options['node_features']:\n",
        "        print(\"### DATA\")\n",
        "        graph, train_graph, train_nodes, test_nodes = get_graph(graph_type,sample_nodes,node_features)\n",
        "        num_features = len(graph.x[0])\n",
        "        for gnn in options['GNN']:\n",
        "          for num_layers in options['num_layers'][gnn]:\n",
        "            for embedding_size in options['embedding_size'][gnn]:\n",
        "              for dropout in options['dropout'][gnn]:\n",
        "                # Defining the neural architecture\n",
        "                hidden_dimension = embedding_size\n",
        "                print(f\"### MODEL: {gnn.__name__} with {num_layers} layers, {num_features} inputs, {hidden_dimension} hidden dimensions, and dropout {dropout}\")\n",
        "                # Doing the proper training\n",
        "                for batch_size in options['batch_size'][sample_nodes]:\n",
        "                  for num_epochs in options[\"num_epochs\"]:\n",
        "                    for train_purity in options['train_purity']:\n",
        "                      for train_sample_ratio in options['train_sample_ratio']:\n",
        "                        for learning_rate in options['learning_rate'][gnn]:\n",
        "                          num_train_examples = round(train_sample_ratio*len(train_nodes))\n",
        "                          model = gnn(num_features,\n",
        "                                      hidden_dimension,\n",
        "                                      hidden_dimension,\n",
        "                                      num_layers=num_layers,\n",
        "                                      dropout=dropout).to(device)\n",
        "                          predictor = MLPNodePredictor(embedding_size=embedding_size).to(device)\n",
        "                          print(model)\n",
        "                          print(predictor)\n",
        "                          # Preparing training\n",
        "                          optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=learning_rate)\n",
        "                          print(f\"### TRAINING with {num_train_examples}+{num_train_examples} ({(100*train_sample_ratio):1f}% sample) train/val examples, purity ({train_purity['min_pure']}, {train_purity['max_spurious']}), batch_size = {batch_size}, num_epochs = {num_epochs}, learning rate = {learning_rate}\")\n",
        "                          start_time = time.time()\n",
        "                          min_loss, min_loss_epoch, max_acc, max_acc_epoch = train(model=model,\n",
        "                                                                                   predictor=predictor,\n",
        "                                                                                   train_graph=train_graph,\n",
        "                                                                                   train_nodes=train_nodes,\n",
        "                                                                                   train_sample_ratio=train_sample_ratio,\n",
        "                                                                                   loss_fn=loss_fn,\n",
        "                                                                                   optimizer=optimizer,\n",
        "                                                                                   batch_size=batch_size,\n",
        "                                                                                   num_epochs=num_epochs,\n",
        "                                                                                   purity=train_purity)\n",
        "                          end_time = time.time()\n",
        "                          train_time = end_time-start_time\n",
        "                          print(f\"Minimum loss:     {min_loss:.4f} reached at epoch {min_loss_epoch}\")\n",
        "                          print(f\"Maximum accuracy: {max_acc:.4f} reached at epoch {max_acc_epoch}\")\n",
        "                          print(f\"Training time: {train_time} s\")\n",
        "                          # Testing the results\n",
        "                          num_test_examples = len(test_nodes)\n",
        "                          for test_purity in options['test_purity']:\n",
        "                            n_tests = 5\n",
        "                            print(f\"\\n### TEST (x{n_tests}) with {num_test_examples}+{num_test_examples} examples and purity ({test_purity['min_pure']}, {test_purity['max_spurious']})\")\n",
        "                            accuracy = [0]*n_tests\n",
        "                            hits = [0]*n_tests\n",
        "                            mrr = [0]*n_tests\n",
        "                            # Loading best model and predictor\n",
        "                            model.load_state_dict(torch.load(dir + 'best_model.pth'))\n",
        "                            predictor.load_state_dict(torch.load(dir + 'best_predictor.pth'))\n",
        "                            for i in range(n_tests):\n",
        "                              accuracy[i], hits[i], mrr[i] = test(model=model,\n",
        "                                                                  predictor=predictor,\n",
        "                                                                  test_graph=graph,\n",
        "                                                                  test_nodes=test_nodes,\n",
        "                                                                  loss_fn=loss_fn,\n",
        "                                                                  num_test_examples=num_test_examples,\n",
        "                                                                  purity=test_purity)\n",
        "                            accuracy_mean = mean(accuracy)\n",
        "                            # neg_sources = neg_edges[0,neg_edges[1,:]==target]\n",
        "                            hits_mean = {}\n",
        "                            for k in hits[0].keys():\n",
        "                              hits_mean[k] = mean([hits[i][k] for i in range(n_tests)])\n",
        "                            mrr_mean = mean(mrr)\n",
        "\n",
        "                            print(f\"\\nAverage values (on {n_tests} test runs)\")\n",
        "                            print(f\"- Accuracy (correct predictions): {accuracy_mean:.4f}%\")\n",
        "                            print(f\"- Hits@k (wrt {num_test_examples} negative examples): \", end=\"\")\n",
        "                            for k in hits[0].keys():\n",
        "                              print(f\"k={k}: {hits_mean[k]:.4f}; \", end=\"\")\n",
        "                            print()\n",
        "                            print(f\"- MRR: {mrr_mean:.4f}\")\n",
        "\n",
        "                            output = {\n",
        "                              \"graph_type\": graph_type,\n",
        "                              \"graph_nodes\": graph.num_nodes,\n",
        "                              \"node_features\": node_features,\n",
        "                              \"gnn\": gnn.__name__,\n",
        "                              \"num_layers\": num_layers,\n",
        "                              \"embedding_size\": embedding_size,\n",
        "                              \"dropout\": dropout,\n",
        "                              \"num_train_examples\": num_train_examples,\n",
        "                              \"train_purity\": f\"({train_purity['min_pure']}, {train_purity['max_spurious']})\",\n",
        "                              \"batch_size\": batch_size,\n",
        "                              \"learning_rate\": learning_rate,\n",
        "                              \"num_epochs\": num_epochs,\n",
        "                              \"min_loss\": min_loss,\n",
        "                              \"min_loss_epoch\": min_loss_epoch,\n",
        "                              \"max_acc\": max_acc,\n",
        "                              \"max_acc_epoch\": max_acc_epoch,\n",
        "                              \"train_time\": train_time,\n",
        "                              \"num_test_examples\": num_test_examples,\n",
        "                              \"test_purity\": f\"({test_purity['min_pure']}, {test_purity['max_spurious']})\",\n",
        "                              \"accuracy_mean\": accuracy_mean,\n",
        "                              \"num_neg_examples\": num_test_examples,\n",
        "                              \"hits_mean1\": hits_mean[1],\n",
        "                              \"hits_mean2\": hits_mean[2],\n",
        "                              \"hits_mean3\": hits_mean[3],\n",
        "                              \"hits_mean5\": hits_mean[5],\n",
        "                              \"hits_mean10\": hits_mean[10],\n",
        "                              \"hits_mean20\": hits_mean[20],\n",
        "                              \"hits_mean30\": hits_mean[30],\n",
        "                              \"hits_mean50\": hits_mean[50],\n",
        "                              \"mrr_mean\": mrr_mean\n",
        "                            }\n",
        "\n",
        "                            write_line_to_xlsx(output)\n",
        "                            print(f\"### Learning task #{n+1} done\\n\\n\")\n",
        "                            n = n+1\n",
        "  this_run = this_run+1\n",
        "print(f'Executed {n} learning tasks')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNJgS4MTg3k8dXuOiEYwtNl",
      "include_colab_link": true,
      "mount_file_id": "148dWjcDDlhQKH9pOllbP2jJewUB9zAjK",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
